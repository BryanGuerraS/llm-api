# ğŸš€ LLM API con FastAPI y Docker ğŸŒŸ
Â¡Bienvenido al proyecto LLM API! ğŸ‰ Este repositorio contiene una implementaciÃ³n de una API que utiliza FastAPI para procesar preguntas, buscar texto relevante y generar respuestas utilizando un modelo de lenguaje grande (LLM). La aplicaciÃ³n se integra con Cohere para la generaciÃ³n de embeddings y ChromaDB para la bÃºsqueda de similitudes.


## ğŸŒŸ CaracterÃ­sticas principales
- ğŸ” Procesa documentos .docx, dividiÃ©ndolos en fragmentos (chunks) y almacenando sus embeddings.  
- ğŸ¤– Genera respuestas concisas y relevantes a preguntas del usuario.  
- ğŸ—„ï¸ Utiliza ChromaDB para buscar documentos similares.  
- ğŸ“¦ Se puede ejecutar en un contenedor Docker para facilitar su despliegue


## ğŸ“‚ Estructura del proyecto
```console
ğŸ“ app  
â”œâ”€â”€ document_processing.py  # Procesa documentos y los divide en chunks.  
â”œâ”€â”€ embeddings.py           # Genera embeddings utilizando Cohere.  
â”œâ”€â”€ storage.py              # Almacena chunks y embeddings en ChromaDB.  
â”œâ”€â”€ search.py               # Busca documentos relevantes en ChromaDB.  
â”œâ”€â”€ generate_answer.py      # Genera respuestas basadas en documentos relevantes.  
ğŸ“ postman                  # Contiene la colecciÃ³n de Postman para probar la API.  
â”œâ”€â”€ LLM_API.postman_collection.json  # ColecciÃ³n de Postman exportada.  
Dockerfile                  # ConfiguraciÃ³n para construir la imagen Docker.  
main.py                     # Archivo principal de la API.  
requirements.txt            # LibrerÃ­as necesarias para el proyecto.
```

## ğŸš€ CÃ³mo ejecutar el proyecto
### 1ï¸âƒ£ Requisitos previos
- ğŸ Python 3.9+  
- ğŸ³ Docker instalado.  
- ğŸ§ª Postman o cualquier cliente HTTP para probar la API.

### 2ï¸âƒ£ EjecuciÃ³n local
1. Clona el repositorio:
```console
git clone https://github.com/tu_usuario/llm-api.git
cd llm-api
```
2. Crea un entorno virtual e instala las dependencias:
```console
python -m venv env
source env/bin/activate  # En Windows: .\env\Scripts\activate
pip install -r requirements.txt
```
3. Ejecuta la aplicaciÃ³n
```console
uvicorn main:app --reload
```
4. Prueba la API
Prueba la API:
- Visita: http://127.0.0.1:8000 para comprobar que la API estÃ¡ activa.
- Usa Postman para probar el endpoint principal /question-llm/.
### 3ï¸âƒ£ EjecuciÃ³n con Docker
1. Construye la imagen Docker:
```console
docker build -t llm-api .
```
2. Ejecuta el contenedor:
```console
docker run -p 8000:8000 llm-api
```
3. Prueba la API:
- La API estarÃ¡ disponible en: http://127.0.0.1:8000.


## ğŸ› ï¸ Endpoints principales
### POST /question-llm/
- Procesa una pregunta y genera una respuesta basada en documentos relevantes.
Request Body:
```console
{
    "user_name": "Bryan_Guerra",
    "question": "Quien es Zara?"
}
```
Respuesta:
```console
{
    "user_name": "Bryan_Guerra",
    "question": "Quien es Zara?",
    "answer": "Zara es un intrÃ©pido explorador en la galaxia de Zenthoria, que descubre un antiguo artefacto clave para la paz entre los Dracorians y los Lumis. ğŸŒŒï¸ğŸ›¡ï¸ğŸ›¸"
}
```

## ğŸ“¦ Postman Collection
Se incluye una colecciÃ³n de Postman en la carpeta postman/ con el archivo LLM_API.postman_collection.json.

CÃ³mo usarla:
1. Abre Postman y selecciona Importar.
2. Carga el archivo postman/LLM_API.postman_collection.json desde tu proyecto.
3. Â¡Prueba los endpoints fÃ¡cilmente! ğŸ‰

## ğŸ“– DocumentaciÃ³n adicional
- Docker: Este proyecto incluye un archivo Dockerfile que permite desplegar la API rÃ¡pidamente en un contenedor.
- Readability: CÃ³digo modular y bien documentado siguiendo buenas prÃ¡cticas.

## ğŸŒ Contribuciones
Â¡Las contribuciones son bienvenidas! Si encuentras un problema o deseas agregar una mejora, por favor abre un issue o envÃ­a un pull request. ğŸ™Œ